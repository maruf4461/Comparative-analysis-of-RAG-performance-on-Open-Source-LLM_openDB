{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMlMcOWrplzs7yNx7eyEzfi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maruf4461/Comparative-analysis-of-RAG-performance-on-Open-Source-LLM_openDB/blob/main/01_Environment_Setup_Complete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Complete RAG Research Implementation\n",
        "# =========================================="
      ],
      "metadata": {
        "id": "4-ykwnDO3RwZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 1: GPU Check and Drive Mount"
      ],
      "metadata": {
        "id": "r6TdkTAg3WRL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VodLrOWw3Nr9",
        "outputId": "00a47300-56b3-466b-bea8-3731a5453f56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍  Checking GPU availability...\n",
            "✅  GPU Available: Tesla T4\n",
            "📊  GPU Memory: 15.8 GB\n",
            "\n",
            "📂  Mounting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅  Project structure created!\n",
            "📁  Project directory: /content/drive/MyDrive/RAG_Research_Complete\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "import sys\n",
        "from google.colab import drive\n",
        "import subprocess\n",
        "\n",
        "print(\"🔍  Checking GPU availability...\")\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name()\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"✅  GPU Available: {gpu_name}\")\n",
        "    print(f\"📊  GPU Memory: {gpu_memory:.1f} GB\")\n",
        "\n",
        "    if gpu_memory < 15:\n",
        "        print(\"⚠️  Warning: Less than 15GB GPU memory. Will use quantization for larger models.\")\n",
        "else:\n",
        "    print(\"❌  No GPU available. Please enable GPU in Runtime > Change runtime type\")\n",
        "    print(\"   Go to Runtime > Change runtime type > Hardware accelerator > GPU\")\n",
        "\n",
        "# Mount Google Drive\n",
        "print(\"\\n📂  Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create comprehensive project structure\n",
        "project_dir = '/content/drive/MyDrive/RAG_Research_Complete'\n",
        "directories = [\n",
        "    'data/raw/msmarco',\n",
        "    'data/raw/natural_questions',\n",
        "    'data/raw/squad',\n",
        "    'data/raw/hotpotqa',\n",
        "    'data/processed/chunks',\n",
        "    'data/processed/embeddings',\n",
        "    'models/llama2_7b',\n",
        "    'models/llama2_13b',\n",
        "    'models/mistral_7b',\n",
        "    'models/codellama_7b',\n",
        "    'models/llama3_8b',\n",
        "    'results/experiments',\n",
        "    'results/analysis',\n",
        "    'results/plots',\n",
        "    'results/tables',\n",
        "    'src/models',\n",
        "    'src/evaluation',\n",
        "    'src/data_processing',\n",
        "    'configs',\n",
        "    'logs',\n",
        "    'checkpoints',\n",
        "    'python_files'\n",
        "]\n",
        "\n",
        "for dir_path in directories:\n",
        "    full_path = os.path.join(project_dir, dir_path)\n",
        "    os.makedirs(full_path, exist_ok=True)\n",
        "\n",
        "print(\"✅  Project structure created!\")\n",
        "print(f\"📁  Project directory: {project_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 2: Install All Required Dependencies"
      ],
      "metadata": {
        "id": "fNam7LLR3Z20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def install_package(package):\n",
        "    \"\"\"Install package with progress tracking\"\"\"\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
        "        return True\n",
        "    except subprocess.CalledProcessError:\n",
        "        return False\n",
        "\n",
        "# Core ML packages\n",
        "core_packages = [\n",
        "    \"transformers>=4.35.0\",\n",
        "    \"accelerate>=0.24.0\",\n",
        "    \"bitsandbytes>=0.41.0\",\n",
        "    \"sentencepiece>=0.1.99\",\n",
        "    \"torch>=2.0.0\",\n",
        "    \"datasets>=2.14.0\"\n",
        "]\n",
        "\n",
        "# RAG specific packages\n",
        "rag_packages = [\n",
        "    \"sentence-transformers>=2.2.2\",\n",
        "    \"chromadb>=0.4.15\",\n",
        "    \"faiss-cpu>=1.7.4\",\n",
        "    \"langchain>=0.0.330\",\n",
        "    \"tiktoken>=0.5.1\"\n",
        "]\n",
        "\n",
        "# Evaluation packages\n",
        "eval_packages = [\n",
        "    \"rouge-score>=0.1.2\",\n",
        "    \"bert-score>=0.3.13\",\n",
        "    \"sacrebleu>=2.3.1\",\n",
        "    \"nltk>=3.8.1\",\n",
        "    \"spacy>=3.7.0\"\n",
        "]\n",
        "\n",
        "# Data processing packages\n",
        "data_packages = [\n",
        "    \"pandas>=2.0.0\",\n",
        "    \"numpy>=1.24.0\",\n",
        "    \"scipy>=1.10.0\",\n",
        "    \"scikit-learn>=1.3.0\",\n",
        "    \"tqdm>=4.65.0\"\n",
        "]\n",
        "\n",
        "# Visualization packages\n",
        "viz_packages = [\n",
        "    \"matplotlib>=3.7.0\",\n",
        "    \"seaborn>=0.12.0\",\n",
        "    \"plotly>=5.17.0\",\n",
        "    \"kaleido>=0.2.1\"\n",
        "]\n",
        "\n",
        "# Statistics packages\n",
        "stats_packages = [\n",
        "    \"statsmodels>=0.14.0\",\n",
        "    \"pingouin>=0.5.3\",\n",
        "    \"scipy>=1.10.0\"\n",
        "]\n",
        "\n",
        "# Utility packages\n",
        "util_packages = [\n",
        "    \"python-dotenv>=1.0.0\",\n",
        "    \"wandb>=0.16.0\",\n",
        "    \"huggingface_hub>=0.17.0\",\n",
        "    \"psutil>=5.9.0\",\n",
        "    \"requests>=2.31.0\"\n",
        "]\n",
        "\n",
        "all_packages = core_packages + rag_packages + eval_packages + data_packages + viz_packages + stats_packages + util_packages\n",
        "\n",
        "print(\"🔧  Installing all required packages...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "failed_packages = []\n",
        "for i, package in enumerate(all_packages):\n",
        "    print(f\"📦  [{i+1}/{len(all_packages)}] Installing {package.split('>=')[0]}...\")\n",
        "    if not install_package(package):\n",
        "        failed_packages.append(package)\n",
        "        print(f\"❌  Failed: {package}\")\n",
        "    else:\n",
        "        print(f\"✅  Success: {package.split('>=')[0]}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "if failed_packages:\n",
        "    print(f\"❌  Failed packages ({len(failed_packages)}):\")\n",
        "    for pkg in failed_packages:\n",
        "        print(f\"   - {pkg}\")\n",
        "else:\n",
        "    print(\"✅  All packages installed successfully!\")\n",
        "\n",
        "# Download required NLTK data\n",
        "import nltk\n",
        "print(\"\\n📚  Downloading NLTK data...\")\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "print(\"✅  NLTK data downloaded\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXZFHbP83Z9k",
        "outputId": "a45f1852-ec0c-404f-83db-ee280423515c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧  Installing all required packages...\n",
            "============================================================\n",
            "📦  [1/33] Installing transformers...\n",
            "✅  Success: transformers\n",
            "📦  [2/33] Installing accelerate...\n",
            "✅  Success: accelerate\n",
            "📦  [3/33] Installing bitsandbytes...\n",
            "✅  Success: bitsandbytes\n",
            "📦  [4/33] Installing sentencepiece...\n",
            "✅  Success: sentencepiece\n",
            "📦  [5/33] Installing torch...\n",
            "✅  Success: torch\n",
            "📦  [6/33] Installing datasets...\n",
            "✅  Success: datasets\n",
            "📦  [7/33] Installing sentence-transformers...\n",
            "✅  Success: sentence-transformers\n",
            "📦  [8/33] Installing chromadb...\n",
            "✅  Success: chromadb\n",
            "📦  [9/33] Installing faiss-cpu...\n",
            "✅  Success: faiss-cpu\n",
            "📦  [10/33] Installing langchain...\n",
            "✅  Success: langchain\n",
            "📦  [11/33] Installing tiktoken...\n",
            "✅  Success: tiktoken\n",
            "📦  [12/33] Installing rouge-score...\n",
            "✅  Success: rouge-score\n",
            "📦  [13/33] Installing bert-score...\n",
            "✅  Success: bert-score\n",
            "📦  [14/33] Installing sacrebleu...\n",
            "✅  Success: sacrebleu\n",
            "📦  [15/33] Installing nltk...\n",
            "✅  Success: nltk\n",
            "📦  [16/33] Installing spacy...\n",
            "✅  Success: spacy\n",
            "📦  [17/33] Installing pandas...\n",
            "✅  Success: pandas\n",
            "📦  [18/33] Installing numpy...\n",
            "✅  Success: numpy\n",
            "📦  [19/33] Installing scipy...\n",
            "✅  Success: scipy\n",
            "📦  [20/33] Installing scikit-learn...\n",
            "✅  Success: scikit-learn\n",
            "📦  [21/33] Installing tqdm...\n",
            "✅  Success: tqdm\n",
            "📦  [22/33] Installing matplotlib...\n",
            "✅  Success: matplotlib\n",
            "📦  [23/33] Installing seaborn...\n",
            "✅  Success: seaborn\n",
            "📦  [24/33] Installing plotly...\n",
            "✅  Success: plotly\n",
            "📦  [25/33] Installing kaleido...\n",
            "✅  Success: kaleido\n",
            "📦  [26/33] Installing statsmodels...\n",
            "✅  Success: statsmodels\n",
            "📦  [27/33] Installing pingouin...\n",
            "✅  Success: pingouin\n",
            "📦  [28/33] Installing scipy...\n",
            "✅  Success: scipy\n",
            "📦  [29/33] Installing python-dotenv...\n",
            "✅  Success: python-dotenv\n",
            "📦  [30/33] Installing wandb...\n",
            "✅  Success: wandb\n",
            "📦  [31/33] Installing huggingface_hub...\n",
            "✅  Success: huggingface_hub\n",
            "📦  [32/33] Installing psutil...\n",
            "✅  Success: psutil\n",
            "📦  [33/33] Installing requests...\n",
            "✅  Success: requests\n",
            "\n",
            "============================================================\n",
            "✅  All packages installed successfully!\n",
            "\n",
            "📚  Downloading NLTK data...\n",
            "✅  NLTK data downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kZGrulOraYnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 3: Create Utility Classes"
      ],
      "metadata": {
        "id": "vQbWUa-r3aT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pickle\n",
        "import time\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Any, Optional\n",
        "import pandas as pd\n",
        "\n",
        "class ProjectUtils:\n",
        "    \"\"\"Comprehensive utility class for RAG research project\"\"\"\n",
        "\n",
        "    def __init__(self, project_dir: str = '/content/drive/MyDrive/RAG_Research_Complete'):\n",
        "        self.project_dir = project_dir\n",
        "        self.logs = []\n",
        "\n",
        "    def log(self, message: str, level: str = \"INFO\"):\n",
        "        \"\"\"Log messages with timestamp\"\"\"\n",
        "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        log_entry = f\"[{timestamp}] {level}: {message}\"\n",
        "        self.logs.append(log_entry)\n",
        "        print(log_entry)\n",
        "\n",
        "        # Save to log file\n",
        "        log_file = os.path.join(self.project_dir, 'logs', 'experiment.log')\n",
        "        with open(log_file, 'a') as f:\n",
        "            f.write(log_entry + '\\n')\n",
        "\n",
        "    def save_data(self, data: Any, filepath: str, format: str = 'json'):\n",
        "        \"\"\"Save data in various formats\"\"\"\n",
        "        full_path = os.path.join(self.project_dir, filepath)\n",
        "        os.makedirs(os.path.dirname(full_path), exist_ok=True)\n",
        "\n",
        "        try:\n",
        "            if format == 'json':\n",
        "                with open(full_path, 'w') as f:\n",
        "                    json.dump(data, f, indent=2, default=str)\n",
        "            elif format == 'pickle':\n",
        "                with open(full_path, 'wb') as f:\n",
        "                    pickle.dump(data, f)\n",
        "            elif format == 'csv':\n",
        "                data.to_csv(full_path, index=False)\n",
        "            elif format == 'parquet':\n",
        "                data.to_parquet(full_path, index=False)\n",
        "\n",
        "            self.log(f\"Saved data to {filepath}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            self.log(f\"Failed to save {filepath}: {e}\", \"ERROR\")\n",
        "            return False\n",
        "\n",
        "    def load_data(self, filepath: str, format: str = 'json'):\n",
        "        \"\"\"Load data in various formats\"\"\"\n",
        "        full_path = os.path.join(self.project_dir, filepath)\n",
        "\n",
        "        try:\n",
        "            if format == 'json':\n",
        "                with open(full_path, 'r') as f:\n",
        "                    return json.load(f)\n",
        "            elif format == 'pickle':\n",
        "                with open(full_path, 'rb') as f:\n",
        "                    return pickle.load(f)\n",
        "            elif format == 'csv':\n",
        "                return pd.read_csv(full_path)\n",
        "            elif format == 'parquet':\n",
        "                return pd.read_parquet(full_path)\n",
        "        except Exception as e:\n",
        "            self.log(f\"Failed to load {filepath}: {e}\", \"ERROR\")\n",
        "            return None\n",
        "\n",
        "    def clear_gpu_memory(self):\n",
        "        \"\"\"Clear GPU memory\"\"\"\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            self.log(\"GPU memory cleared\")\n",
        "\n",
        "    def get_system_info(self):\n",
        "        \"\"\"Get comprehensive system information\"\"\"\n",
        "        import psutil\n",
        "\n",
        "        info = {\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'gpu_available': torch.cuda.is_available(),\n",
        "            'gpu_count': torch.cuda.device_count() if torch.cuda.is_available() else 0,\n",
        "            'cpu_count': psutil.cpu_count(),\n",
        "            'memory_total_gb': psutil.virtual_memory().total / 1e9,\n",
        "            'memory_available_gb': psutil.virtual_memory().available / 1e9,\n",
        "            'disk_free_gb': psutil.disk_usage('/content').free / 1e9\n",
        "        }\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            info['gpu_name'] = torch.cuda.get_device_name()\n",
        "            info['gpu_memory_gb'] = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "            info['gpu_memory_allocated_gb'] = torch.cuda.memory_allocated() / 1e9\n",
        "\n",
        "        return info\n",
        "\n",
        "    def save_checkpoint(self, data: Dict[str, Any], name: str):\n",
        "        \"\"\"Save experiment checkpoint\"\"\"\n",
        "        timestamp = int(time.time())\n",
        "        checkpoint_data = {\n",
        "            'timestamp': timestamp,\n",
        "            'datetime': datetime.now().isoformat(),\n",
        "            'system_info': self.get_system_info(),\n",
        "            'data': data\n",
        "        }\n",
        "\n",
        "        filepath = f\"checkpoints/{name}_{timestamp}.json\"\n",
        "        return self.save_data(checkpoint_data, filepath, 'json')\n",
        "\n",
        "    def load_latest_checkpoint(self, name_pattern: str):\n",
        "        \"\"\"Load most recent checkpoint matching pattern\"\"\"\n",
        "        checkpoint_dir = os.path.join(self.project_dir, 'checkpoints')\n",
        "        if not os.path.exists(checkpoint_dir):\n",
        "            return None\n",
        "\n",
        "        files = [f for f in os.listdir(checkpoint_dir)\n",
        "                if name_pattern in f and f.endswith('.json')]\n",
        "\n",
        "        if not files:\n",
        "            return None\n",
        "\n",
        "        # Sort by timestamp in filename\n",
        "        latest_file = max(files, key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
        "        return self.load_data(f\"checkpoints/{latest_file}\", 'json')\n",
        "\n",
        "# Initialize utils\n",
        "utils = ProjectUtils()\n",
        "utils.log(\"Project utilities initialized\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFuY5SRv3aap",
        "outputId": "56c0d6cf-f0e0-4ae5-cab7-0d2ec7f37a15"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-06-24 10:12:17] INFO: Project utilities initialized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 4: Test Setup and Save Configuration"
      ],
      "metadata": {
        "id": "2DpxJZ_N3ahf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the setup\n",
        "system_info = utils.get_system_info()\n",
        "utils.log(\"System information collected\")\n",
        "\n",
        "# Save system configuration\n",
        "config = {\n",
        "    'project_name': 'RAG_Research_Complete',\n",
        "    'setup_timestamp': datetime.now().isoformat(),\n",
        "    'system_info': system_info,\n",
        "    'models_to_evaluate': [\n",
        "        'meta-llama/Llama-2-7b-chat-hf',\n",
        "        'meta-llama/Llama-2-13b-chat-hf',\n",
        "        'mistralai/Mistral-7B-Instruct-v0.1',\n",
        "        'codellama/CodeLlama-7b-Instruct-hf',\n",
        "        'meta-llama/Meta-Llama-3-8B-Instruct'\n",
        "    ],\n",
        "    'datasets': [\n",
        "        'ms_marco',\n",
        "        'natural_questions',\n",
        "        'squad_v2',\n",
        "        'hotpot_qa'\n",
        "    ],\n",
        "    'rag_configurations': [\n",
        "        'basic_rag',\n",
        "        'enhanced_rag',\n",
        "        'optimized_rag'\n",
        "    ],\n",
        "    'evaluation_metrics': [\n",
        "        'rouge_l',\n",
        "        'bleu',\n",
        "        'bert_score',\n",
        "        'recall_at_k',\n",
        "        'mrr',\n",
        "        'ndcg'\n",
        "    ]\n",
        "}\n",
        "\n",
        "utils.save_data(config, 'configs/project_config.json')\n",
        "\n",
        "# Print setup summary\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"🎯  RAG RESEARCH PROJECT SETUP COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "print(f\"📁  Project Directory: {utils.project_dir}\")\n",
        "print(f\"🖥   GPU: {system_info.get('gpu_name', 'Not available')}\")\n",
        "print(f\"💾  GPU Memory: {system_info.get('gpu_memory_gb', 0):.1f} GB\")\n",
        "print(f\"🧠  RAM: {system_info['memory_total_gb']:.1f} GB\")\n",
        "print(f\"💿  Disk Free: {system_info['disk_free_gb']:.1f} GB\")\n",
        "print(f\"📊  Models to test: {len(config['models_to_evaluate'])}\")\n",
        "print(f\"📚  Datasets: {len(config['datasets'])}\")\n",
        "print(f\"⚙️   RAG configs: {len(config['rag_configurations'])}\")\n",
        "print(f\"📈  Metrics: {len(config['evaluation_metrics'])}\")\n",
        "print(\"\\n✅  Ready to proceed to Phase 2: Data Preparation\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWtCsxRn3aoQ",
        "outputId": "8ac30dd7-c1bb-4e02-f913-d13f8c839278"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-06-24 10:12:27] INFO: System information collected\n",
            "[2025-06-24 10:12:27] INFO: Saved data to configs/project_config.json\n",
            "\n",
            "================================================================================\n",
            "🎯  RAG RESEARCH PROJECT SETUP COMPLETE\n",
            "================================================================================\n",
            "📁  Project Directory: /content/drive/MyDrive/RAG_Research_Complete\n",
            "🖥   GPU: Tesla T4\n",
            "💾  GPU Memory: 15.8 GB\n",
            "🧠  RAM: 13.6 GB\n",
            "💿  Disk Free: 75.6 GB\n",
            "📊  Models to test: 5\n",
            "📚  Datasets: 4\n",
            "⚙️   RAG configs: 3\n",
            "📈  Metrics: 6\n",
            "\n",
            "✅  Ready to proceed to Phase 2: Data Preparation\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2bQP4YL53at2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hhutOG8C3a3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GFemOA1c3bCF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z_-ZHPfX3bNl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}