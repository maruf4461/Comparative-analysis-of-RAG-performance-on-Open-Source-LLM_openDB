{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNX7ZffnQhdVpDPLHZHVG2Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maruf4461/Comparative-analysis-of-RAG-performance-on-Open-Source-LLM_openDB/blob/main/utils.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "RAG Research Project Utilities\n",
        "Comprehensive utility functions for multi-model RAG evaluation\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import pickle\n",
        "import time\n",
        "import logging\n",
        "import traceback\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Any, Optional, Union\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import gc\n",
        "import psutil\n",
        "from pathlib import Path\n",
        "\n",
        "class ProjectUtils:\n",
        "    \"\"\"Comprehensive utility class for RAG research project\"\"\"\n",
        "\n",
        "    def __init__(self, project_dir: str = '/content/drive/MyDrive/RAG_Research_Complete'):\n",
        "        self.project_dir = project_dir\n",
        "        self.logs = []\n",
        "        self.setup_logging()\n",
        "        self.ensure_project_structure()\n",
        "\n",
        "    def setup_logging(self):\n",
        "        \"\"\"Setup comprehensive logging\"\"\"\n",
        "        log_dir = os.path.join(self.project_dir, 'logs')\n",
        "        os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "        # Create logger\n",
        "        self.logger = logging.getLogger('RAGResearch')\n",
        "        self.logger.setLevel(logging.DEBUG)\n",
        "\n",
        "        # Create file handler\n",
        "        log_file = os.path.join(log_dir, f'rag_research_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log')\n",
        "        file_handler = logging.FileHandler(log_file)\n",
        "        file_handler.setLevel(logging.DEBUG)\n",
        "\n",
        "        # Create console handler\n",
        "        console_handler = logging.StreamHandler()\n",
        "        console_handler.setLevel(logging.INFO)\n",
        "\n",
        "        # Create formatter\n",
        "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "        file_handler.setFormatter(formatter)\n",
        "        console_handler.setFormatter(formatter)\n",
        "\n",
        "        # Add handlers to logger\n",
        "        if not self.logger.handlers:\n",
        "            self.logger.addHandler(file_handler)\n",
        "            self.logger.addHandler(console_handler)\n",
        "\n",
        "    def ensure_project_structure(self):\n",
        "        \"\"\"Ensure all project directories exist\"\"\"\n",
        "        directories = [\n",
        "            'data/raw/msmarco',\n",
        "            'data/raw/natural_questions',\n",
        "            'data/raw/squad',\n",
        "            'data/raw/hotpotqa',\n",
        "            'data/processed/chunks',\n",
        "            'data/processed/embeddings',\n",
        "            'models/llama2_7b',\n",
        "            'models/llama2_13b',\n",
        "            'models/mistral_7b',\n",
        "            'models/codellama_7b',\n",
        "            'models/llama3_8b',\n",
        "            'results/experiments',\n",
        "            'results/analysis',\n",
        "            'results/plots',\n",
        "            'results/tables',\n",
        "            'src/models',\n",
        "            'src/evaluation',\n",
        "            'src/data_processing',\n",
        "            'configs',\n",
        "            'logs',\n",
        "            'checkpoints'\n",
        "        ]\n",
        "\n",
        "        for directory in directories:\n",
        "            full_path = os.path.join(self.project_dir, directory)\n",
        "            os.makedirs(full_path, exist_ok=True)\n",
        "\n",
        "    def log(self, message: str, level: str = \"INFO\"):\n",
        "        \"\"\"Log messages with timestamp\"\"\"\n",
        "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        log_entry = f\"[{timestamp}] {level}: {message}\"\n",
        "        self.logs.append(log_entry)\n",
        "\n",
        "        # Use logger\n",
        "        if level.upper() == \"ERROR\":\n",
        "            self.logger.error(message)\n",
        "        elif level.upper() == \"WARNING\":\n",
        "            self.logger.warning(message)\n",
        "        elif level.upper() == \"DEBUG\":\n",
        "            self.logger.debug(message)\n",
        "        else:\n",
        "            self.logger.info(message)\n",
        "\n",
        "        # Also print to console with emoji\n",
        "        emoji_map = {\n",
        "            \"ERROR\": \"âŒ\",\n",
        "            \"WARNING\": \"âš ï¸\",\n",
        "            \"INFO\": \"â„¹ï¸\",\n",
        "            \"DEBUG\": \"ðŸ›\",\n",
        "            \"SUCCESS\": \"âœ…\"\n",
        "        }\n",
        "        emoji = emoji_map.get(level.upper(), \"ðŸ“\")\n",
        "        print(f\"{emoji} {message}\")\n",
        "\n",
        "    def save_data(self, data: Any, filepath: str, format: str = 'json'):\n",
        "        \"\"\"Save data in various formats\"\"\"\n",
        "        full_path = os.path.join(self.project_dir, filepath)\n",
        "        os.makedirs(os.path.dirname(full_path), exist_ok=True)\n",
        "\n",
        "        try:\n",
        "            if format == 'json':\n",
        "                with open(full_path, 'w', encoding='utf-8') as f:\n",
        "                    json.dump(data, f, indent=2, default=str, ensure_ascii=False)\n",
        "            elif format == 'pickle':\n",
        "                with open(full_path, 'wb') as f:\n",
        "                    pickle.dump(data, f)\n",
        "            elif format == 'csv':\n",
        "                if isinstance(data, pd.DataFrame):\n",
        "                    data.to_csv(full_path, index=False)\n",
        "                elif isinstance(data, str):\n",
        "                    with open(full_path, 'w', encoding='utf-8') as f:\n",
        "                        f.write(data)\n",
        "                else:\n",
        "                    pd.DataFrame(data).to_csv(full_path, index=False)\n",
        "            elif format == 'parquet':\n",
        "                if isinstance(data, pd.DataFrame):\n",
        "                    data.to_parquet(full_path, index=False)\n",
        "                else:\n",
        "                    pd.DataFrame(data).to_parquet(full_path, index=False)\n",
        "            elif format == 'txt':\n",
        "                with open(full_path, 'w', encoding='utf-8') as f:\n",
        "                    if isinstance(data, str):\n",
        "                        f.write(data)\n",
        "                    else:\n",
        "                        f.write(str(data))\n",
        "\n",
        "            self.log(f\"Saved data to {filepath}\", \"SUCCESS\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            self.log(f\"Failed to save {filepath}: {e}\", \"ERROR\")\n",
        "            self.logger.exception(\"Save data error:\")\n",
        "            return False\n",
        "\n",
        "    def load_data(self, filepath: str, format: str = 'json'):\n",
        "        \"\"\"Load data in various formats\"\"\"\n",
        "        full_path = os.path.join(self.project_dir, filepath)\n",
        "\n",
        "        if not os.path.exists(full_path):\n",
        "            self.log(f\"File not found: {filepath}\", \"WARNING\")\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            if format == 'json':\n",
        "                with open(full_path, 'r', encoding='utf-8') as f:\n",
        "                    return json.load(f)\n",
        "            elif format == 'pickle':\n",
        "                with open(full_path, 'rb') as f:\n",
        "                    return pickle.load(f)\n",
        "            elif format == 'csv':\n",
        "                return pd.read_csv(full_path)\n",
        "            elif format == 'parquet':\n",
        "                return pd.read_parquet(full_path)\n",
        "            elif format == 'txt':\n",
        "                with open(full_path, 'r', encoding='utf-8') as f:\n",
        "                    return f.read()\n",
        "\n",
        "        except Exception as e:\n",
        "            self.log(f\"Failed to load {filepath}: {e}\", \"ERROR\")\n",
        "            self.logger.exception(\"Load data error:\")\n",
        "            return None\n",
        "\n",
        "    def clear_gpu_memory(self):\n",
        "        \"\"\"Clear GPU memory comprehensively\"\"\"\n",
        "        if torch.cuda.is_available():\n",
        "            # Clear PyTorch cache\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            # Force garbage collection\n",
        "            gc.collect()\n",
        "\n",
        "            # Clear CUDA context if needed\n",
        "            try:\n",
        "                torch.cuda.synchronize()\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            self.log(\"GPU memory cleared\", \"SUCCESS\")\n",
        "\n",
        "            # Log memory status\n",
        "            if torch.cuda.is_available():\n",
        "                memory_allocated = torch.cuda.memory_allocated() / 1e9\n",
        "                memory_reserved = torch.cuda.memory_reserved() / 1e9\n",
        "                self.log(f\"GPU Memory - Allocated: {memory_allocated:.2f}GB, Reserved: {memory_reserved:.2f}GB\")\n",
        "\n",
        "    def get_system_info(self):\n",
        "        \"\"\"Get comprehensive system information\"\"\"\n",
        "        try:\n",
        "            info = {\n",
        "                'timestamp': datetime.now().isoformat(),\n",
        "                'gpu_available': torch.cuda.is_available(),\n",
        "                'gpu_count': torch.cuda.device_count() if torch.cuda.is_available() else 0,\n",
        "                'cpu_count': psutil.cpu_count(),\n",
        "                'memory_total_gb': psutil.virtual_memory().total / 1e9,\n",
        "                'memory_available_gb': psutil.virtual_memory().available / 1e9,\n",
        "                'memory_percent': psutil.virtual_memory().percent,\n",
        "                'disk_free_gb': psutil.disk_usage('/content' if os.path.exists('/content') else '/').free / 1e9,\n",
        "                'python_version': sys.version,\n",
        "                'torch_version': torch.__version__ if torch else 'Not available'\n",
        "            }\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                info['gpu_name'] = torch.cuda.get_device_name()\n",
        "                info['gpu_memory_total_gb'] = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "                info['gpu_memory_allocated_gb'] = torch.cuda.memory_allocated() / 1e9\n",
        "                info['gpu_memory_reserved_gb'] = torch.cuda.memory_reserved() / 1e9\n",
        "\n",
        "            return info\n",
        "\n",
        "        except Exception as e:\n",
        "            self.log(f\"Error getting system info: {e}\", \"ERROR\")\n",
        "            return {'error': str(e)}\n",
        "\n",
        "    def save_checkpoint(self, data: Dict[str, Any], name: str):\n",
        "        \"\"\"Save experiment checkpoint with metadata\"\"\"\n",
        "        timestamp = int(time.time())\n",
        "        checkpoint_data = {\n",
        "            'checkpoint_name': name,\n",
        "            'timestamp': timestamp,\n",
        "            'datetime': datetime.now().isoformat(),\n",
        "            'system_info': self.get_system_info(),\n",
        "            'data': data,\n",
        "            'logs_snapshot': self.logs[-10:] if len(self.logs) > 10 else self.logs  # Last 10 logs\n",
        "        }\n",
        "\n",
        "        filepath = f\"checkpoints/{name}_{timestamp}.json\"\n",
        "        success = self.save_data(checkpoint_data, filepath, 'json')\n",
        "\n",
        "        if success:\n",
        "            self.log(f\"Checkpoint saved: {name}\", \"SUCCESS\")\n",
        "\n",
        "        return success\n",
        "\n",
        "    def load_latest_checkpoint(self, name_pattern: str):\n",
        "        \"\"\"Load most recent checkpoint matching pattern\"\"\"\n",
        "        checkpoint_dir = os.path.join(self.project_dir, 'checkpoints')\n",
        "\n",
        "        if not os.path.exists(checkpoint_dir):\n",
        "            self.log(f\"Checkpoint directory not found: {checkpoint_dir}\", \"WARNING\")\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            files = [f for f in os.listdir(checkpoint_dir)\n",
        "                    if name_pattern in f and f.endswith('.json')]\n",
        "\n",
        "            if not files:\n",
        "                self.log(f\"No checkpoints found matching pattern: {name_pattern}\", \"WARNING\")\n",
        "                return None\n",
        "\n",
        "            # Sort by timestamp in filename\n",
        "            latest_file = max(files, key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
        "\n",
        "            self.log(f\"Loading checkpoint: {latest_file}\", \"SUCCESS\")\n",
        "            return self.load_data(f\"checkpoints/{latest_file}\", 'json')\n",
        "\n",
        "        except Exception as e:\n",
        "            self.log(f\"Error loading checkpoint: {e}\", \"ERROR\")\n",
        "            return None\n",
        "\n",
        "    def monitor_resources(self):\n",
        "        \"\"\"Monitor system resources\"\"\"\n",
        "        try:\n",
        "            # CPU and Memory\n",
        "            cpu_percent = psutil.cpu_percent(interval=1)\n",
        "            memory = psutil.virtual_memory()\n",
        "\n",
        "            # GPU if available\n",
        "            gpu_info = \"\"\n",
        "            if torch.cuda.is_available():\n",
        "                gpu_memory = torch.cuda.memory_allocated() / 1e9\n",
        "                gpu_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "                gpu_percent = (gpu_memory / gpu_total) * 100\n",
        "                gpu_info = f\", GPU: {gpu_percent:.1f}% ({gpu_memory:.1f}/{gpu_total:.1f}GB)\"\n",
        "\n",
        "            self.log(f\"Resources - CPU: {cpu_percent:.1f}%, RAM: {memory.percent:.1f}%{gpu_info}\")\n",
        "\n",
        "            # Warning thresholds\n",
        "            if memory.percent > 90:\n",
        "                self.log(\"High memory usage detected!\", \"WARNING\")\n",
        "            if torch.cuda.is_available() and (torch.cuda.memory_allocated() / torch.cuda.get_device_properties(0).total_memory) > 0.9:\n",
        "                self.log(\"High GPU memory usage detected!\", \"WARNING\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.log(f\"Error monitoring resources: {e}\", \"ERROR\")\n",
        "\n",
        "    def handle_exception(self, e: Exception, context: str = \"\"):\n",
        "        \"\"\"Handle exceptions with comprehensive logging\"\"\"\n",
        "        error_msg = f\"Exception in {context}: {str(e)}\"\n",
        "        self.log(error_msg, \"ERROR\")\n",
        "\n",
        "        # Log full traceback\n",
        "        self.logger.error(f\"Full traceback for {context}:\")\n",
        "        self.logger.error(traceback.format_exc())\n",
        "\n",
        "        # Save error report\n",
        "        error_report = {\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'context': context,\n",
        "            'error_message': str(e),\n",
        "            'error_type': type(e).__name__,\n",
        "            'traceback': traceback.format_exc(),\n",
        "            'system_info': self.get_system_info()\n",
        "        }\n",
        "\n",
        "        error_filename = f\"error_report_{int(time.time())}.json\"\n",
        "        self.save_data(error_report, f\"logs/{error_filename}\")\n",
        "\n",
        "        return error_report\n",
        "\n",
        "    def create_experiment_config(self, models: List[str], rag_configs: List[str],\n",
        "                               datasets: List[str], metrics: List[str]):\n",
        "        \"\"\"Create comprehensive experiment configuration\"\"\"\n",
        "        config = {\n",
        "            'project_name': 'RAG_Research_Complete',\n",
        "            'created_timestamp': datetime.now().isoformat(),\n",
        "            'models': {\n",
        "                'llama2_7b': {\n",
        "                    'model_name': 'meta-llama/Llama-2-7b-chat-hf',\n",
        "                    'display_name': 'LLaMA 2 7B',\n",
        "                    'max_memory_gb': 14,\n",
        "                    'use_8bit': True,\n",
        "                    'model_type': 'llama2'\n",
        "                },\n",
        "                'llama2_13b': {\n",
        "                    'model_name': 'meta-llama/Llama-2-13b-chat-hf',\n",
        "                    'display_name': 'LLaMA 2 13B',\n",
        "                    'max_memory_gb': 26,\n",
        "                    'use_8bit': True,\n",
        "                    'model_type': 'llama2'\n",
        "                },\n",
        "                'mistral_7b': {\n",
        "                    'model_name': 'mistralai/Mistral-7B-Instruct-v0.1',\n",
        "                    'display_name': 'Mistral 7B',\n",
        "                    'max_memory_gb': 14,\n",
        "                    'use_8bit': True,\n",
        "                    'model_type': 'mistral'\n",
        "                },\n",
        "                'codellama_7b': {\n",
        "                    'model_name': 'codellama/CodeLlama-7b-Instruct-hf',\n",
        "                    'display_name': 'CodeLLaMA 7B',\n",
        "                    'max_memory_gb': 14,\n",
        "                    'use_8bit': True,\n",
        "                    'model_type': 'codellama'\n",
        "                },\n",
        "                'llama3_8b': {\n",
        "                    'model_name': 'meta-llama/Meta-Llama-3-8B-Instruct',\n",
        "                    'display_name': 'LLaMA 3 8B',\n",
        "                    'max_memory_gb': 16,\n",
        "                    'use_8bit': True,\n",
        "                    'model_type': 'llama3'\n",
        "                }\n",
        "            },\n",
        "            'datasets': {\n",
        "                'msmarco': {\n",
        "                    'name': 'MS MARCO',\n",
        "                    'type': 'passage_ranking',\n",
        "                    'size_limit': 100000,\n",
        "                    'chunk_size': 256,\n",
        "                    'overlap': 25\n",
        "                },\n",
        "                'natural_questions': {\n",
        "                    'name': 'Natural Questions',\n",
        "                    'type': 'question_answering',\n",
        "                    'size_limit': 10000,\n",
        "                    'chunk_size': 512,\n",
        "                    'overlap': 50\n",
        "                },\n",
        "                'squad': {\n",
        "                    'name': 'SQuAD 2.0',\n",
        "                    'type': 'reading_comprehension',\n",
        "                    'size_limit': 20000,\n",
        "                    'chunk_size': 512,\n",
        "                    'overlap': 50\n",
        "                },\n",
        "                'hotpotqa': {\n",
        "                    'name': 'HotpotQA',\n",
        "                    'type': 'multi_hop_qa',\n",
        "                    'size_limit': 10000,\n",
        "                    'chunk_size': 512,\n",
        "                    'overlap': 50\n",
        "                }\n",
        "            },\n",
        "            'rag_configurations': {\n",
        "                'basic': {\n",
        "                    'name': 'Basic RAG',\n",
        "                    'description': 'Top-3 retrieved documents with simple concatenation',\n",
        "                    'top_k': 3,\n",
        "                    'rerank': False,\n",
        "                    'query_expansion': False\n",
        "                },\n",
        "                'enhanced': {\n",
        "                    'name': 'Enhanced RAG',\n",
        "                    'description': 'Top-5 documents with relevance scoring and filtering',\n",
        "                    'top_k': 5,\n",
        "                    'rerank': True,\n",
        "                    'query_expansion': False,\n",
        "                    'relevance_threshold': 0.5\n",
        "                },\n",
        "                'optimized': {\n",
        "                    'name': 'Optimized RAG',\n",
        "                    'description': 'Dynamic document selection with query analysis',\n",
        "                    'top_k': 'dynamic',\n",
        "                    'rerank': True,\n",
        "                    'query_expansion': True,\n",
        "                    'query_analysis': True\n",
        "                }\n",
        "            },\n",
        "            'evaluation_metrics': {\n",
        "                'generation_quality': ['rouge1_f', 'rouge2_f', 'rougeL_f', 'bleu', 'bert_score_f1'],\n",
        "                'retrieval_quality': ['recall_at_k', 'mrr', 'ndcg'],\n",
        "                'semantic_quality': ['semantic_similarity'],\n",
        "                'efficiency': ['retrieval_time', 'generation_time', 'total_time']\n",
        "            },\n",
        "            'vector_databases': ['chromadb', 'faiss'],\n",
        "            'embedding_model': 'all-MiniLM-L6-v2',\n",
        "            'experiment_settings': {\n",
        "                'max_new_tokens': 256,\n",
        "                'temperature': 0.7,\n",
        "                'batch_size': 1,\n",
        "                'max_experiments_per_session': 100,\n",
        "                'checkpoint_frequency': 10\n",
        "            }\n",
        "        }\n",
        "\n",
        "        self.save_data(config, 'configs/project_config.json')\n",
        "        self.log(\"Experiment configuration created\", \"SUCCESS\")\n",
        "        return config\n",
        "\n",
        "    def validate_environment(self):\n",
        "        \"\"\"Validate that all required components are available\"\"\"\n",
        "        validation_results = {\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'checks': {},\n",
        "            'overall_status': True\n",
        "        }\n",
        "\n",
        "        # Check GPU availability\n",
        "        gpu_available = torch.cuda.is_available()\n",
        "        validation_results['checks']['gpu'] = {\n",
        "            'available': gpu_available,\n",
        "            'count': torch.cuda.device_count() if gpu_available else 0,\n",
        "            'name': torch.cuda.get_device_name() if gpu_available else None,\n",
        "            'memory_gb': torch.cuda.get_device_properties(0).total_memory / 1e9 if gpu_available else 0\n",
        "        }\n",
        "\n",
        "        if not gpu_available:\n",
        "            self.log(\"GPU not available - some operations may be slow\", \"WARNING\")\n",
        "\n",
        "        # Check memory\n",
        "        memory = psutil.virtual_memory()\n",
        "        memory_gb = memory.total / 1e9\n",
        "        validation_results['checks']['memory'] = {\n",
        "            'total_gb': memory_gb,\n",
        "            'available_gb': memory.available / 1e9,\n",
        "            'adequate': memory_gb >= 12  # Minimum for model loading\n",
        "        }\n",
        "\n",
        "        if memory_gb < 12:\n",
        "            self.log(\"Low system memory - may cause issues with larger models\", \"WARNING\")\n",
        "            validation_results['overall_status'] = False\n",
        "\n",
        "        # Check disk space\n",
        "        disk_free = psutil.disk_usage('/content' if os.path.exists('/content') else '/').free / 1e9\n",
        "        validation_results['checks']['disk'] = {\n",
        "            'free_gb': disk_free,\n",
        "            'adequate': disk_free >= 20  # Minimum for datasets and models\n",
        "        }\n",
        "\n",
        "        if disk_free < 20:\n",
        "            self.log(\"Low disk space - may not be sufficient for all datasets\", \"WARNING\")\n",
        "            validation_results['overall_status'] = False\n",
        "\n",
        "        # Check project structure\n",
        "        required_dirs = ['data', 'models', 'results', 'configs', 'logs', 'checkpoints']\n",
        "        missing_dirs = []\n",
        "        for dir_name in required_dirs:\n",
        "            dir_path = os.path.join(self.project_dir, dir_name)\n",
        "            if not os.path.exists(dir_path):\n",
        "                missing_dirs.append(dir_name)\n",
        "\n",
        "        validation_results['checks']['project_structure'] = {\n",
        "            'all_present': len(missing_dirs) == 0,\n",
        "            'missing_directories': missing_dirs\n",
        "        }\n",
        "\n",
        "        if missing_dirs:\n",
        "            self.log(f\"Missing directories: {missing_dirs}\", \"WARNING\")\n",
        "            # Try to create them\n",
        "            self.ensure_project_structure()\n",
        "\n",
        "        # Save validation results\n",
        "        self.save_data(validation_results, 'logs/environment_validation.json')\n",
        "\n",
        "        if validation_results['overall_status']:\n",
        "            self.log(\"Environment validation passed\", \"SUCCESS\")\n",
        "        else:\n",
        "            self.log(\"Environment validation completed with warnings\", \"WARNING\")\n",
        "\n",
        "        return validation_results\n",
        "\n",
        "    def get_progress_summary(self):\n",
        "        \"\"\"Get summary of project progress\"\"\"\n",
        "        summary = {\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'phases': {}\n",
        "        }\n",
        "\n",
        "        # Check each phase\n",
        "        phases = {\n",
        "            'setup': 'configs/project_config.json',\n",
        "            'data_preparation': 'data/processing_summary.json',\n",
        "            'model_implementation': 'results/model_loading_stats.json',\n",
        "            'rag_pipeline': 'results/rag_pipeline_test.json',\n",
        "            'evaluation': 'results/experiments/complete_results.json',\n",
        "            'analysis': 'results/analysis/comprehensive_statistical_report.md'\n",
        "        }\n",
        "\n",
        "        for phase_name, indicator_file in phases.items():\n",
        "            file_exists = os.path.exists(os.path.join(self.project_dir, indicator_file))\n",
        "            summary['phases'][phase_name] = {\n",
        "                'completed': file_exists,\n",
        "                'indicator_file': indicator_file\n",
        "            }\n",
        "\n",
        "        # Calculate overall progress\n",
        "        completed_phases = sum(1 for phase in summary['phases'].values() if phase['completed'])\n",
        "        total_phases = len(summary['phases'])\n",
        "        summary['overall_progress'] = {\n",
        "            'completed_phases': completed_phases,\n",
        "            'total_phases': total_phases,\n",
        "            'percentage': (completed_phases / total_phases) * 100\n",
        "        }\n",
        "\n",
        "        self.log(f\"Project progress: {completed_phases}/{total_phases} phases completed\", \"INFO\")\n",
        "        return summary\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"Get or create project configuration\"\"\"\n",
        "        config_path = 'configs/project_config.json'\n",
        "\n",
        "        # Try to load existing config\n",
        "        existing_config = self.load_data(config_path)\n",
        "\n",
        "        if existing_config is not None:\n",
        "            self.log(\"Loaded existing project configuration\", \"SUCCESS\")\n",
        "            return existing_config\n",
        "\n",
        "        # Create default config if none exists\n",
        "        self.log(\"No existing config found, creating default configuration\")\n",
        "\n",
        "        default_config = self.create_experiment_config(\n",
        "            models=['llama2_7b', 'llama2_13b', 'mistral_7b', 'codellama_7b', 'llama3_8b'],\n",
        "            rag_configs=['basic', 'enhanced', 'optimized'],\n",
        "            datasets=['msmarco', 'natural_questions', 'squad', 'hotpotqa'],\n",
        "            metrics=['rouge1_f', 'rouge2_f', 'rougeL_f', 'bleu', 'bert_score_f1', 'recall_at_k', 'mrr', 'ndcg']\n",
        "        )\n",
        "\n",
        "        return default_config\n",
        "\n",
        "    def initialize_project(self):\n",
        "        \"\"\"Initialize the complete project structure and configuration\"\"\"\n",
        "        self.log(\"Initializing RAG Research Project\", \"INFO\")\n",
        "\n",
        "        # Ensure directory structure\n",
        "        self.ensure_project_structure()\n",
        "\n",
        "        # Validate environment\n",
        "        validation = self.validate_environment()\n",
        "\n",
        "        # Create/load configuration\n",
        "        config = self.get_config()\n",
        "\n",
        "        # Log initialization complete\n",
        "        self.log(\"Project initialization complete\", \"SUCCESS\")\n",
        "\n",
        "        return {\n",
        "            'config': config,\n",
        "            'validation': validation,\n",
        "            'project_dir': self.project_dir,\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "# Create a global utils instance\n",
        "utils = None\n",
        "\n",
        "def get_utils(project_dir: str = '/content/drive/MyDrive/RAG_Research_Complete'):\n",
        "    \"\"\"Get or create global utils instance\"\"\"\n",
        "    global utils\n",
        "    if utils is None:\n",
        "        utils = ProjectUtils(project_dir)\n",
        "    return utils\n",
        "\n",
        "# Convenience functions\n",
        "def log(message: str, level: str = \"INFO\"):\n",
        "    \"\"\"Convenience function for logging\"\"\"\n",
        "    global utils\n",
        "    if utils is None:\n",
        "        utils = get_utils()\n",
        "    utils.log(message, level)\n",
        "\n",
        "def save_data(data: Any, filepath: str, format: str = 'json'):\n",
        "    \"\"\"Convenience function for saving data\"\"\"\n",
        "    global utils\n",
        "    if utils is None:\n",
        "        utils = get_utils()\n",
        "    return utils.save_data(data, filepath, format)\n",
        "\n",
        "def load_data(filepath: str, format: str = 'json'):\n",
        "    \"\"\"Convenience function for loading data\"\"\"\n",
        "    global utils\n",
        "    if utils is None:\n",
        "        utils = get_utils()\n",
        "    return utils.load_data(filepath, format)"
      ],
      "metadata": {
        "id": "dM00yRXucSZu"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}